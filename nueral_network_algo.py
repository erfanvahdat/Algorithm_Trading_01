# -*- coding: utf-8 -*-
"""Nueral_network_algo.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10wTE4pODsVc19jZDQEGpiJ6fY6nIWiPc
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from importlib import import_module  

# Preprocessing Modules
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler

# Torch Modules 
from torch import nn
import torch.nn.functional as F
import torch
from torch.optim import Adam

"""###Preporcessing(Main_class)"""

class Main(object):

  def __init__(self,X,Y):    
    super().__init__()
    self.X=X
    self.Y=Y
    self.condition = True # " True" Or "False"
      
    # if self.condition:
        # self.torch = import_module('torch') #import torch inside the main Class
      
  def SLP(self):
    # Split the Dataset for Traning and Testing
    X_train,X_test,y_train,y_test=train_test_split(self.X,self.Y, train_size=0.8)
    
    # Standardization on X_trian,X_test
    scaler=StandardScaler()
    encoder=OrdinalEncoder()
  
    # Also can written in this form : 
    """
      ->Becareful with the shape of input for standardizaiton.0D is not allowed.
        it will Raise an Error for shape of the data. solution: y.reshape(-1,1)
        or if our Data is tensor use this: torch.reshape(y, (-1,1) ) 

        scaler=StandardScaler.fit(y)
        X_train,X_test=scaler.transform(y)
    """
    try:
      X_train=scaler.fit_transform(X_train)
      X_test=scaler.fit_transform(X_test)
      X_train=torch.tensor(X_train)
      X_test=torch.tensor(X_test)    
    except: 
      print(f'Data didn"t convert to Tensor.\n'
            f'Something wrong with the shape.Please reshape and then Try....')

    # Decoting the categorical columns on y
    if isinstance(self.Y[1], str):
      y_train=encoder.fit_transform(y_train.reshape(-1,1))
      y_test=encoder.fit_transform(y_test.reshape(-1,1))
      y_train=torch.LongTensor(y_train)
      y_test=torch.LongTensor(y_test)

    
    # If target has more than 
    if len(self.X.shape)>1:
      
      # Type of problem
      if  isinstance(self.Y[1], str) | self.X.shape[1]> 1 :
        print('-> Multinomial Logistic Regression <-')

      # If data is less than one column and are not Categorical we define it as Linear and standard the y_columns for getting more Efficient result on testing
      elif  isinstance(self.Y[1], str)| self.X.shape[1]<1:
        print('->Binomial Logistic Regression <-')
      else: 
        print('-> Linear Regression Problem <-')
        y_train=scaler.fit_transform(y_train)
        y_train=torch.tensor(y_train)
        y_test=scaler.fit_transform(y_test)
        y_test=torch.tensor(y_test)


    return (X_train,X_test,y_train,y_test)
    
  def ploting(self,**kwargs): 
    # Print('ploting selected data')
    colors={}

    class_num=len(np.unique(self.Y))
    # Generate color for each class in Data
    for i in np.unique(self.Y):
      random_color=tuple(np.random.random(size=(class_num) ))
      colors.update({f'{i}':random_color})
    
    if  not isinstance(self.Y[1], str):
      plt.scatter(kwargs.get('X'),kwargs.get('y'))
    else:
    # Ploting X values with random color for each values
      plt.scatter(kwargs.get('X'),kwargs.get('y'),c=[colors[i][0] for i in self.Y],s=30,edgecolor='k')
    # plt.title(kwargs.get('title'))
    # plt.xlabel(kwargs.get('xlabel'))
    # plt.legend(kwargs.get('legend'))
    plt.show()

  #Checking for more information about data type and type of our Data
  def status(self):
    print(f'X_train: {type(X_train)} --> {X_train.dtype} \n'
          f'X_test: {type(X_test)} --> {X_test.dtype} \n \n'
          f'y_train: {type(y_train)} --> {y_train.dtype} \n'
          f'y_test: {type(y_test)} --> {y_test.dtype}'    )


"""###Model"""

class Lin2(nn.Module):
  
  def __init__(self,input_dim,output_dim):
      super().__init__()
      self.input_dim=input_dim
      self.output_dim=output_dim

      # Number of the classes
      classes_num=len(np.unique(y_train))

      self.layers = nn.Sequential(
      nn.Linear(self.input_dim, self.output_dim),
      nn.Tanh(), # <- add activation function
      nn.Linear(self.output_dim, self.output_dim),
      # nn.Tanh(),
      nn.Linear(self.output_dim, classes_num),
      # nn.ReLU()
      )    
     
      # Also we can use another form of using Linear Layers
      """
      self.lin1 = nn.Linear(self.input_dim,self.output_dim)
      self.lin2 = nn.Linear(self.output_dim,classes_num)
      """

  #Loss Function for categorical problem
  loss_fn = nn.CrossEntropyLoss()

  # Forwarding the X_train to the Neural Network 
  def forward(self, x):
      return self.layers(x)

  # For categorical problem, we sum all the True label with pred label to validate how much of are the labels that predicted were True?
  def accuracy_fn(self,**kwargs):
    n_correct = torch.eq(kwargs.get('pred'), kwargs.get('y_true')).sum().item()
    accuracy = (n_correct / len(kwargs.get('pred'))) * 100
    return accuracy

# Define our Model



"""###Training"""

# Training
def training(X_train,X_test,y_train,y_test):
  
  optimizer = Adam(model_1.parameters(), lr=0.2)

  for epoch in range(100):
    # Forward pass
    y_pred = model_1(X_train.float())
    
    # Loss function
    loss = model_1.loss_fn(y_pred, y_train.squeeze())
    
    
    # Zero all gradients
    optimizer.zero_grad()

    # Backward pass
    loss.backward()

    # Update weights
    optimizer.step()

    if epoch%5==0:
      predictions = y_pred.max(dim=1)[1] # class
      # print(predictions.shape,y_train.squeeze().shape)
      """
      All the tensor must has the same Dim.If Dim aren't the same the 
      accuracy we would be have a problem.Use reshape method instead.
      """
      
      accuracy = model_1.accuracy_fn(pred=predictions,y_true=y_train.squeeze())
      
      print (f"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}")


def plot_multiclass_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))
    cmap = plt.cm.Spectral
    
    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()
    y_pred = F.softmax(model(X_test), dim=1) 
    _, y_pred = y_pred.max(dim=1)
    y_pred = y_pred.reshape(xx.shape)
    plt.figure(figsize=(10,5))
    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("Train")
    plt.xlabel('X')
    plt.ylabel('Y')

