{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è¨ Data quality\n"
      ],
      "metadata": {
        "id": "oSvBnXTCXMrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "from module_list import *"
      ],
      "metadata": {
        "id": "RlGPMPQevhMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tumors.csv\"\n",
        "df=pd.read_csv(url,header=0)\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "df.head(2)\n",
        "# Define X and y  \n",
        "X = df[[\"leukocyte_count\", \"blood_pressure\"]].values\n",
        "y = df[\"tumor_class\"].values\n",
        "print (\"X: \", np.shape(X))\n",
        "print (\"y: \", np.shape(y))"
      ],
      "metadata": {
        "id": "zkks2hFZvhZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=np.unique(y)\n",
        "import collections\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "# bb=LabelEncoder.fit(y_train)\n",
        "# encoder=OrdinalEncoder()\n",
        "label_encoder=LabelEncoder().fit(y.reshape(-1,1))\n",
        "# encoder.fit_transform(y.reshape(-1,1))\n",
        "\n",
        "list(label_encoder.classes_),list(collections.Counter(y))\n"
      ],
      "metadata": {
        "id": "Z426heAS62Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.plotting.scatter_matrix(df, figsize=(5, 5));\n",
        "df.corr()"
      ],
      "metadata": {
        "id": "AunuBrKDvhns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ch2UJwhcYyMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GJ5CBXOJlqj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/tumors.csv\"\n",
        "df=pd.read_csv(url,header=0)\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "df.head(2)\n",
        "# Define X and y  \n",
        "X = df[[\"leukocyte_count\", \"blood_pressure\"]].values\n",
        "y = df[\"tumor_class\"].values\n",
        "print (\"X: \", np.shape(X))\n",
        "print (\"y: \", np.shape(y))\n",
        "\n",
        "main=Main(X,y)\n",
        "main.SLP()\n",
        "main.ploting(X=X[:,0],y=X[:,1])\n",
        "# main.status()\n",
        "#modeling\n",
        "model_2=Lin2(2,10)\n",
        "\n",
        "#training\n",
        "training(X_train,X_test,y_train,y_test,model_2)\n",
        "plt.scatter(df['leukocyte_count'].mean(),np.mean(df.blood_pressure), s=200,\n",
        "            c=\"b\", edgecolor=\"w\", linewidth=2)\n",
        "plt.scatter(X[:,0],X[:,1],c='red',alpha=0.2)\n",
        "ax = plt.axes()\n",
        "ax.set_facecolor('#1CC4AF')\n",
        "# Annotate\n",
        "plt.annotate(\"true: malignant,\\npred: malignant\",\n",
        "        color=\"black\",\n",
        "        xy=(df['leukocyte_count'].mean(),np.mean(df.blood_pressure)),\n",
        "        xytext=(0.4, 0.75),\n",
        "        textcoords=\"figure fraction\",\n",
        "        fontsize=12,\n",
        "        arrowprops=dict(facecolor=\"white\", shrink=0.01))\n",
        "\n"
      ],
      "metadata": {
        "id": "BGONXOtdAURM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3mxNsHE-JbKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from module_list import *\n",
        "seed=1234\n",
        "def set_seed(seed):\n",
        "  np.random.seed(seed);\n",
        "  random.seed(seed);\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed);\n",
        "  torch.cuda.manual_seed_all(seed);\n",
        "set_seed(seed)\n",
        "#set device\n",
        "cuda=True\n",
        "device=torch.device('cuda' if(torch.cuda.is_available() and cuda)else 'cpu')\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "if device.type =='cuda':\n",
        "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "print(device)\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/spiral.csv\"\n",
        "df=pd.read_csv(url,header=0)\n",
        "df=df.sample(frac=1).reset_index(drop=True)\n",
        "X=df[['X1','X2']].values\n",
        "y=df['color'].values\n"
      ],
      "metadata": {
        "id": "Xaak_1s-LOC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main=Main(X,y)\n",
        "main.SLP()\n",
        "main.ploting(X=X[:,0],\n",
        "             y=X[:,1])"
      ],
      "metadata": {
        "id": "5VrB4as1qO8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì¢\n",
        "###DATALOADER"
      ],
      "metadata": {
        "id": "UeWa49cLfjIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create dataloaders\n",
        "set_seed(seed)\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.y)\n",
        "\n",
        "  def __str__(self):\n",
        "        return f\"<Dataset(N={len(self)})>\"\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.X[index]\n",
        "    y = self.y[index]\n",
        "    return [X, y]\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "        \"\"\"Processing on a batch.\"\"\"\n",
        "        # Get inputs\n",
        "        batch = np.array(batch)\n",
        "        X = np.stack(batch[:, 0], axis=0)\n",
        "        y = batch[:, 1]\n",
        "        # Cast\n",
        "        X = torch.FloatTensor(X.astype(np.float32))\n",
        "        y = torch.LongTensor(y.astype(np.int32))\n",
        "        return X, y\n",
        "\n",
        "  def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
        "        shuffle=shuffle, drop_last=drop_last, pin_memory=True)\n",
        "\n",
        "batch_size=64\n",
        "training_data=Dataset(X=X_train.cpu(),y=y_train.cpu())\n",
        "test_data=Dataset(X=X_test,y=y_test)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "train_dataloader2=training_data.create_dataloader(batch_size=batch_size)\n",
        "test_dataloader2=test_data.create_dataloader(batch_size=batch_size)\n",
        "# batch_X,batch_y=next(iter(train_dataloader ))\n",
        "# batch_X_test,batch_y_test=next(iter(test_dataloader))\n",
        "# train_stackedloader=np.stack([batch_X[:,0],batch_X[:,1],batch_y]).T\n",
        "# test_stackedloader=np.stack([batch_X_test[:,0],batch_X_test[:,1],batch_y]).T\n",
        "\n",
        "len(train_dataloader), len(test_dataloader),len(train_dataloader2),len(test_dataloader2)\n"
      ],
      "metadata": {
        "id": "a_egrEjQLOHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚è∞\n",
        "###__MODEL__"
      ],
      "metadata": {
        "id": "hnMiVQ_IfLxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(2,100),\n",
        "      nn.ReLU(),\n",
        "      # nn.Dropout(p=0.2),\n",
        "      nn.Linear(100,100),\n",
        "\n",
        "      # nn.ReLU(),\n",
        "      nn.Linear(100, 3),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    x=self.layers(x)\n",
        "    return x\n",
        "\n",
        "# model_2=MLP(hidden_dim=50,dropout_p=0.2)\n",
        "\n",
        "model_2=MLP()\n"
      ],
      "metadata": {
        "id": "WzA503GIQbdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùé\n",
        "###TRAINING AND TEST"
      ],
      "metadata": {
        "id": "KB2MN6A-fVnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model_2.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.1, patience=3)\n",
        "\n",
        "class Trainer(object):\n",
        "  def __init__(self, model, device, loss_fn, optimizer,EPOCHS,scheduler=None):\n",
        "\n",
        "    super(Trainer, self).__init__()\n",
        "    # Set params\n",
        "    self.model = model\n",
        "    self.device = device\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimizer = optimizer\n",
        "    self.scheduler = scheduler\n",
        "    self.EPOCHS=EPOCHS\n",
        "\n",
        "  def train_step(self,dataloader):\n",
        "\n",
        "    \"\"\"Train step.\"\"\"\n",
        "    # Set model to train mode\n",
        "    size=len(dataloader.dataset) # Size of Training Data\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    self.model.train()\n",
        "    loss,correct=0,0\n",
        "    # Iterate over train batches\n",
        "    for i,(X,y) in enumerate(dataloader):        \n",
        "      X,y=X.to(device),y.to(device)\n",
        "\n",
        "      z=self.model(X.float())\n",
        "      # print(z.shape,y.shape)\n",
        "      # break\n",
        "      FORD = self.loss_fn(z, y.squeeze())\n",
        "      loss += FORD.detach().item()\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      FORD.backward()\n",
        "      self.optimizer.step()\n",
        "      correct+=(z.argmax(1)==y).type(torch.float).sum().item()\n",
        "\n",
        "    loss /= num_batches\n",
        "    correct /= size # Correct is checked with every data point in data but loss is calculated wiht every per Batch.\n",
        "    return correct\n",
        "    #     batch = [item.to(self.device) for item in batch]  # Set device\n",
        "    #     inputs, targets = batch[:-1], batch[-1]\n",
        "    #     self.optimizer.zero_grad()  # Reset gradients\n",
        "    #     FORD = self.model(inputs)  # Forward pass\n",
        "    #     J_loss = self.loss_fn(FORD, targets)  # Define loss\n",
        "    #     J_loss.backward()  # Backward pass\n",
        "    #     self.optimizer.step()  # Update weights\n",
        "\n",
        "    #     # Cumulative Metrics\n",
        "    #     loss += (J_loss.detach().item() - loss) / (i + 1)\n",
        "    # return loss\n",
        "\n",
        "  def eval_step(self, dataloader):\n",
        "    \"\"\"Validation or test step.\"\"\"\n",
        "    # Set model to eval mode\n",
        "    self.model.eval()\n",
        "    size=len(dataloader.dataset) # Size of Training Data\n",
        "    num_batches = len(dataloader)\n",
        "    correct,loss = 0.0, 0.0\n",
        "    # y_trues, y_probs = [], []\n",
        "\n",
        "    # Iterate over val batches\n",
        "    for i in range(self.EPOCHS):\n",
        "      with torch.inference_mode():\n",
        "        for batch,(X,y) in enumerate(dataloader):\n",
        "          X,y=X.to(device),y.to(device)\n",
        "          # Step\n",
        "          z=self.model(X.float())\n",
        "          loss +=self.loss_fn(z,y.squeeze()).item()\n",
        "          # correct+=(z.argmax(1)==y).type(torch.float).sum().item()\n",
        "          # correct=torch.eq(z)\n",
        "          correct+=torch.eq(z.argmax(1),y.reshape(1,-1)).sum().item()\n",
        "        \n",
        "        \n",
        "        \n",
        "      \n",
        "      correct/= size\n",
        "      loss/= num_batches\n",
        "      if i%30==0:\n",
        "        print(f'Epochs {i} | Evaulate_loss {loss:.2f} | Accuracy: {correct*100:2f} ')\n",
        "    return loss\n",
        "\n",
        "  def Train(self,train_dataloader, test_dataloader):\n",
        "    correct=self.train_step(dataloader=train_dataloader)\n",
        "    FORD=self.eval_step(dataloader=test_dataloader)\n",
        "      \n",
        "trainer=Trainer(model=model_2,device=device,loss_fn=loss_fn,\n",
        "                optimizer=optimizer,scheduler=None,EPOCHS=200)\n",
        "\n",
        "# trainer.train_step(dataloader=train_dataloader)\n",
        "trainer.Train(train_dataloader=train_dataloader,test_dataloader=test_dataloader)"
      ],
      "metadata": {
        "id": "Ig6dNFsosc-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NB8TYKWItHF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HYr4vFrtSld6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HmAP7RKHSlkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyLrvIBmtHIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "def get_metrics(y_pred, y_true, classes):\n",
        "    \"\"\"Per-class performance metrics.\"\"\"\n",
        "    # Performance\n",
        "    performance = {\"overall\": {}, \"class\": {}}\n",
        "\n",
        "    metrics = precision_recall_fscore_support(y_pred,y_true.reshape(-1,1))\n",
        "    performance[\"overall\"][\"precision\"] = metrics[0][1]\n",
        "    performance[\"overall\"][\"recall\"] = metrics[1][1]\n",
        "    performance[\"overall\"][\"f1\"] = metrics[2][1]\n",
        "    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n",
        "\n",
        "    # Per-class performance\n",
        "    \n",
        "    return performance\n",
        "\n",
        "\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "\n",
        "\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "y_pred.shape\n",
        "# performance = get_metrics(y_true=y_test, y_pred=y_pred, classes=3)\n",
        "\n",
        "# print (json.dumps(performance[\"overall\"], indent=2))\n"
      ],
      "metadata": {
        "id": "5fgGhix269nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHVka1bm69so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBNoWPsI69xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YzAgS8k692u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfh637bkqFva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iuJauOfEJbPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîí\n",
        "**`OLD CLASS`**\n",
        "1. **MAIN class for preprocessing Data**\n",
        "2. **MODEL Class**\n",
        "3. **Testing *italicized text* and Training**"
      ],
      "metadata": {
        "id": "EE7H9wvjgLfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from module_list import *\n",
        "class Main(object):\n",
        "  \"\"\"\n",
        "    Preprocessing Data in SLP;\n",
        "     Modeling in Lin2;\n",
        "     Training in Training\n",
        "  \"\"\"\n",
        "  def __init__(self,X,Y):    \n",
        "    super().__init__()\n",
        "    self.X=X\n",
        "    self.Y=Y\n",
        "    self.condition = True # \" True\" Or \"False\"\n",
        "      \n",
        "    # if self.condition:\n",
        "        # self.torch = import_module('torch') #import torch inside the main Class\n",
        "      \n",
        "  def SLP(self):\n",
        "    global X_train,X_test,y_train,y_test,y_1\n",
        "    # Split the Dataset for Traning and Testing\n",
        "    y_1=self.Y\n",
        "    X_train,X_test,y_train,y_test=train_test_split(self.X,self.Y, train_size=0.8)\n",
        "    \n",
        "    # Standardization on X_trian,X_test\n",
        "    scaler=StandardScaler()\n",
        "    encoder=OrdinalEncoder()\n",
        "    \n",
        "    # Also can written in this form : \n",
        "    \"\"\"\n",
        "      ->Becareful with the shape of input for standardizaiton.0D is not allowed.\n",
        "        it will Raise an Error for shape of the data. solution: y.reshape(-1,1)\n",
        "        or if our Data is tensor use this: torch.reshape(y, (-1,1) ) \n",
        "\n",
        "        scaler=StandardScaler.fit(y)\n",
        "        X_train,X_test=scaler.transform(y)\n",
        "    \"\"\"\n",
        "    try:\n",
        "      X_train=scaler.fit_transform(X_train)\n",
        "      X_test=scaler.fit_transform(X_test)\n",
        "      X_train=torch.tensor(X_train)\n",
        "      X_test=torch.tensor(X_test)    \n",
        "    except: \n",
        "      print(f'Data didn\"t convert to Tensor.\\n'\n",
        "            f'Something wrong with the shape.Please reshape and then Try....')\n",
        "\n",
        "    # Decoting the categorical columns on y\n",
        "    if isinstance(self.Y[1], str):\n",
        "      y_train=encoder.fit_transform(y_train.reshape(-1,1))\n",
        "      y_test=encoder.fit_transform(y_test.reshape(-1,1))\n",
        "      y_train=torch.LongTensor(y_train)\n",
        "      y_test=torch.LongTensor(y_test)\n",
        "\n",
        "    \n",
        "    # If target has more than \n",
        "    if len(self.X.shape)>1:\n",
        "      \n",
        "      # Type of problem\n",
        "      if  isinstance(self.Y[1], str) | self.X.shape[1]> 1 :\n",
        "        print('-> Multinomial Logistic Regression <-')\n",
        "\n",
        "      # If data is less than one column and are not Categorical we define it as Linear and standard the y_columns for getting more Efficient result on testing\n",
        "      elif  isinstance(self.Y[1], str)| self.X.shape[1]<1:\n",
        "        print('->Binomial Logistic Regression <-')\n",
        "      else: \n",
        "        print('-> Linear Regression Problem <-')\n",
        "        y_train=scaler.fit_transform(y_train)\n",
        "        y_train=torch.tensor(y_train)\n",
        "        y_test=scaler.fit_transform(y_test)\n",
        "        y_test=torch.tensor(y_test)\n",
        "    \n",
        "    # return (X_train,X_test,y_train,y_test)\n",
        "    \n",
        "  def ploting(self,**kwargs): \n",
        "    # Print('ploting selected data')\n",
        "    \n",
        "    colors={}\n",
        "    \n",
        "    class_num=len(np.unique(self.Y))\n",
        "    # Generate color for each class in Dat\n",
        "    colors={}\n",
        "    for i in np.unique(self.Y):\n",
        "      rand1=tuple(np.random.random(size=3))\n",
        "      colors.update({f'{i}':rand1})\n",
        "\n",
        "    plt.scatter(self.X[:, 0],\n",
        "                self.X[:, 1], c=[colors[_y] for _y in y_1],\n",
        "            s=25, edgecolors=\"k\")\n",
        "\n",
        "    # plt.title(kwargs.get('title'))\n",
        "    # plt.xlabel(kwargs.get('xlabel'))\n",
        "    # plt.legend(kwargs.get('legend'))\n",
        "    plt.show()\n",
        "\n",
        "  #Checking for more information about data type and type of our Data\n",
        "  def status(self):\n",
        "    print(f'X_train: {type(X_train)} --> {X_train.dtype} \\n'\n",
        "          f'X_test: {type(X_test)} --> {X_test.dtype} \\n \\n'\n",
        "          f'y_train: {type(y_train)} --> {y_train.dtype} \\n'\n",
        "          f'y_test: {type(y_test)} --> {y_test.dtype}'    )\n",
        "    \n",
        "\n",
        "class Lin2(nn.Module):  \n",
        "  def __init__(self,input_dim,output_dim):\n",
        "      super().__init__()\n",
        "      \"\"\"hi\"\"\"\n",
        "      self.input_dim=input_dim\n",
        "      self.output_dim=output_dim\n",
        "\n",
        "      # Number of the classes\n",
        "      classes_num=len(np.unique(y_train))\n",
        "\n",
        "      self.layers = nn.Sequential(\n",
        "      nn.Linear(self.input_dim, self.output_dim),\n",
        "      nn.Tanh(), # <- add activation function\n",
        "      nn.Linear(self.output_dim, self.output_dim),\n",
        "      # nn.Tanh(),\n",
        "      nn.Linear(self.output_dim, classes_num),\n",
        "      # nn.ReLU()\n",
        "      )    \n",
        "     \n",
        "      # Also we can use another form of using Linear Layers\n",
        "      \"\"\"\n",
        "      self.lin1 = nn.Linear(self.input_dim,self.output_dim)\n",
        "      self.lin2 = nn.Linear(self.output_dim,classes_num)\n",
        "      \"\"\"\n",
        "\n",
        "  #Loss Function for categorical problem\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Forwarding the X_train to the Neural Network \n",
        "  def forward(self, x):\n",
        "      return self.layers(x)\n",
        "\n",
        "  # For categorical problem, we sum all the True label with pred label to validate how much of are the labels that predicted were True?\n",
        "  def accuracy_fn(self,**kwargs):\n",
        "    n_correct = torch.eq(kwargs.get('pred'), kwargs.get('y_true')).sum().item()\n",
        "    accuracy = (n_correct / len(kwargs.get('pred'))) * 100\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Training\n",
        "def training(X_train,X_test,y_train,y_test,model):\n",
        "  \"\"\"\"X_train,X_test,y_train,y_test,model\"\"\"\n",
        "  optimizer = Adam(model.parameters(), lr=0.2)\n",
        "\n",
        "  for epoch in range(100):\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train.float())\n",
        "    \n",
        "    # Loss function\n",
        "    loss = model.loss_fn(y_pred, y_train.squeeze())\n",
        "    \n",
        "    \n",
        "    # Zero all gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch%20==0:\n",
        "      predictions = y_pred.max(dim=1)[1] # class\n",
        "      # print(predictions.shape,y_train.squeeze().shape)\n",
        "      \"\"\"\n",
        "      All the tensor must has the same Dim.If Dim aren't the same the \n",
        "      accuracy we would be have a problem.Use reshape method instead.\n",
        "      \"\"\"\n",
        "      \n",
        "      accuracy = model.accuracy_fn(pred=predictions,y_true=y_train.squeeze())\n",
        "      \n",
        "      print (f\"Epoch: {epoch} | loss: {loss:.2f}, accuracy: {accuracy:.1f}\")\n",
        "\n",
        "\n",
        "def plot_multiclass_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
        "    cmap = plt.cm.Spectral\n",
        "    \n",
        "    X_test = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
        "    y_pred = F.softmax(model(X_test), dim=1) \n",
        "    _, y_pred = y_pred.max(dim=1)\n",
        "    y_pred = y_pred.reshape(xx.shape)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.title(\"Train\")\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    "
      ],
      "metadata": {
        "id": "MP8vEf1Kl2oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bV2H6psx1zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g723IHP7x137"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzF_avMOx19L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5zZu7kRx2Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pc7EmRzx2bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4UdDXzSx2iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0cOrvRG_Njj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZNCFckA_No8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exuNFRl7_Nua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Get the class of categorical columns\n",
        "   1. label_encoder=Labelencoder().fit(y)\n",
        "      list(label_encoder.classses_)\n",
        "  2. list(colleciton.Counter(y))\n",
        "   \"\"\"\n",
        "\n",
        "   #example of labelencoding; -> two ways\n",
        "c={'bird','animal','sea','cat'}\n",
        "\"\"\"  set are not Ordered   \"\"\"\n",
        "\n",
        "print(\n",
        "    list((LabelEncoder().fit(list(c))).classes_)\n",
        "    )\n",
        "print(\n",
        "  list(collections.Counter(list(c) ))  \n",
        "  )\n",
        "\n",
        "\n",
        "\"\"\" We CANT global var of a used var, Global var must first empty\"\"\"\n",
        "\"\"\" Reset all variable---> %reset or %reset -f\"\"\""
      ],
      "metadata": {
        "id": "QXLh3KMax2n0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}